<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Randomized Overdrive Neural Networks">
    <meta name="author" content="Christian Steinmetz,
                                Joshua D. Reiss">

    <title>Randomized Overdrive Neural Networks</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

</head>

<body>
<bibtex src="neurips/references.bib"></bibtex>
<div class="jumbotron jumbotron-fluid text-center">
    <div class="container"></div>
    <h1>Randomized Overdrive Neural Networks</h1>
    <p style="margin-bottom: 0;">
        <a href="http://www.christiansteinmetz.com"> Christian J. Steinmetz</a> and
        <a href="http://www.eecs.qmul.ac.uk/~josh/"> Joshua D. Reiss</a>
    </p>
    <p style="margin-top: 0;">
        Queen Mary University of London
    </p>

    <div class="btn-group center" role="group" aria-label="Top menu">
        <a class="btn btn-secondary" target="blank_" href="https://arxiv.org">Paper</a>
        <a class="btn btn-secondary" target="blank_" href="https://github.com/csteinmetz1/ronn">Code</a>
        <a class="btn btn-secondary" target="blank_" href="https://drive.google.com/file/d/15tA3X21N5FhLsDvElGBArUFA9cTogDLL/view?usp=sharing">VST/AU</a>
    </div>
</div>

<div class="container" style="max-width: 950px; margin-left: auto; margin-right: auto;">
    <div class="container">
        <div class="section">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe 
                width="560" 
                height="315" 
                src="https://www.youtube.com/embed/s1p_CvwDEB8" 
                frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                allowfullscreen></iframe>
            </div>
            <hr>
            <p>
                By processing audio signals in the time-domain with randomly weighted temporal convolutional networks (TCNs),
                we uncover a wide range of novel, yet controllable overdrive effects.
                We discover that architectural aspects, such as the depth of the network, 
                the kernel size, the number of channels, the activation function, as well as the weight initialization, 
                all have a clear impact on the sonic character of the resultant effect, without the need for training. 
                In practice, these effects range from conventional overdrive and distortion,
                to more extreme effects, as the receptive field grows, similar to a fusion of distortion, equalization, delay, and reverb.  
                To enable use by musicians and producers, we provide a real-time plugin implementation.
                This allows users to dynamically design networks, listening to the results in real-time.
            </p>
        </div>
    
        <div class="section">
            <h2>History</h2>
            <hr>
            <p>
            Distortion was first discovered by guitarists when they found that pushing their guitar amplifiers beyond their operating range produced a warm and fuzzy sound.

            </p>
            <p>
                <div class="row">
                    <div class="col-md-6">
                        <a href="#">
                            <img class="img-fluid" src="neurips/figs/old-amp-front.jpg" alt="inn_logo">
                        </a>
                    </div>
                    <div class="col-md-6">
                        <a href="#">
                            <img class="img-fluid" src="neurips/figs/old-amp-back.jpg" alt="ccs_logo">
                        </a>
                    </div>
                </div>
                <blockquote class="blockquote">
                    <footer class="blockquote-footer">1940 National Dobro Corp Model 75 Vintage Tube Guitar Amplifier</footer>
                </blockquote>
            </p>
            <p>
                <div class="row">
                    <div class="col-md-12">
                        <img class="img-fluid" src='neurips/figs/hard_clipper.png'>
                        <blockquote class="blockquote">
                        <footer class="blockquote-footer">Simple diode clipper distortion circuit. Source: <a href=https://www.strymon.net/single-stage-multistage-gain-topologies-sunset-riverside/>strymon.net</a></footer>
                        </blockquote>
                    </div>
                </div>
            </p>         
        </div>

        <div class="section">
            <h2>Architecture</h2>
            <hr>
            <p>
                The TCN architecture has already been shown to be successful in modeling sequential data across a number of domains, 
                with WaveNet being one of the early examples in the audio domain (). More recent works 
            </p>
            <img width="300px" class="center" src='neurips/figs/plugin-diagram-v2.svg'>
        </div>

        <div class="section">
            <h2>Plugin</h2>
            <hr>
            <p>
                The goal of the plugin is to build a C++ implementation that enables users to quickly and easily construct different
                randomly weighted TCNs, and listen to the produced effects in real-time. 
                This shifts the process of searching for and selecting audio processing effects from the traditional paradigm 
                where users adjust the controls of traditional DSP devices like equalizers and dynamic range compressors,
                to one where users adjust the architecture of a neural network in order to search for new effects. 
                This is true since the TCN can be viewed as a generalized audio effect that can effectively implement a range of transforms
                similar to those traditionally employed by audio engineers (e.g. equalization, dynamic range compression, delay, reverberation). 
            </p>
            <img width="600px" src='neurips/figs/ronn-vst-ui.png'>
        </div>
    
        <div class="section">
            <h2>Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on using neural networks in audio effects! <br>
            </p>
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='https://csteinmetz1.github.io/NeuralReverberator/slides/static/img/nr-rir.gif' class='img-fluid'>
                </div>
    
                <div class="col">
                    <div class='paper-title'>
                        <a href="https://www.christiansteinmetz.com/projects-blog/neuralreverberator">NeuralReverberator</a>
                    </div>
                    <div>
                        Adding reverberation to musical sources is an essential part of creating an engaging listening experience. 
                        NeuralReverberator is a convolutional reverb synthesizer that uses a spectral autoencoder as a generative model. 
                        We implement the model as a real-time plugin that allows audio engineers to traverse the latent space of the autoencoder,
                        and generate novel reverberation effect with varying timbre and duration. 
                    </div>
                </div>
            </div>
            <br>
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='https://floweq.ml/img/tf.gif' class='img-fluid'>
                </div>
    
                <div class="col">
                    <div class='paper-title'>
                        <a href="http://floweq.ml">flowEQ</a>
                    </div>
                    <div>
                        We use a disentangled variational autoencoder (β-VAE) in order to provide a new modality for modifying the timbre of recordings via a parametric equalizer. 
                        By traversing the learned latent space of the trained decoder network in the real-time plugin, 
                        the user can more quickly search through the configurations of a five band parametric equalizer. 
                        This methodology promotes using one’s ears to determine the proper EQ settings over looking at transfer functions or specific frequency controls.
                    </div>
                </div>
            </div>
        </div>
        <br>
    
        <div class="section">
            <h2>Paper</h2>
            <hr>
            <div>
                <div class="list-group">
                    <a href="https:/arxiv.org"
                       class="list-group-item">
                        <img src="neurips/figs/paper_thumbnail_sm.jpg" style="width:100%; margin-right:-20px; margin-top:-10px;">
                    </a>
                </div>
            </div>
        </div>
    
        <div class="section">
            <h2>Bibtex</h2>
            <hr>
            <div style="background-color: #e9ecef;">
                <pre>
                <code>
    @article{steinmetz2020overdrive,
            title={Randomized Overdrive Neural Networks},
            author={Steinmetz, Christian J. and Reiss, Joshua D.},
            journal={arXiv preprint arXiv:},
            year={2020}}</code>
                </pre>
            </div>
        </div>

        <!-- <div class="section">
            <h2 id="bibliography">Bibliography</h2>
            <hr>
        
            <div id="bibtex_display"></div>
            <div class="bibtex_template">
                <p>
                    <div class="if author">
                    <span class="author"></span>
                    </div>
                    <span class="if title">
                    <a class="url">
                        <span class="title"></span>
                    </a>
                    </span>
                    <div>
                    <span class="if journal"><em><span class="journal"></span></em></span>
                    <span class="if month"><span class="month"></span>,</span>
                    <span class="if year"><span class="year"></span>.</span>
                    </div>
                </p>
            </div>
        </div>  -->

        <hr>
        <footer>
            <p>*Submitted to the NeurIPS 2020 Workshop on Machine Learning for Creativity and Design</p>
            <p>Send feedback and questions to <a href="http://christiansteinmetz.com">Christian Steinmetz</a>.</p>
        </footer>
    </div>
</div>

<script type="text/javascript" src="https://cdn.rawgit.com/pcooksey/bibtex-js/5ccf967/src/bibtex_js.js"></script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
