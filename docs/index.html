<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Randomized Overdrive Neural Networks">
    <meta name="author" content="Christian Steinmetz,
                                Joshua D. Reiss">

    <title>Randomized Overdrive Neural Networks</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

</head>

<body>
<bibtex src="neurips/references.bib"></bibtex>
<div class="jumbotron jumbotron-fluid text-center">
    <div class="container"></div>
    <h1>Randomized Overdrive Neural Networks</h1>
    <p style="margin-bottom: 0;">
        <a href="http://www.christiansteinmetz.com"> Christian J. Steinmetz</a> and
        <a href="http://www.eecs.qmul.ac.uk/~josh/"> Joshua D. Reiss</a>
    </p>
    <p style="margin-top: 0;">
        Queen Mary University of London
    </p>

    <div class="btn-group center" role="group" aria-label="Top menu">
        <a class="btn btn-secondary" target="blank_" href="">Paper</a>
        <a class="btn btn-secondary" target="blank_" href="https://github.com/csteinmetz1/ronn">Code</a>
        <a class="btn btn-secondary" target="blank_" href="https://drive.google.com/file/d/15tA3X21N5FhLsDvElGBArUFA9cTogDLL/view?usp=sharing">VST/AU</a>
    </div>
</div>

<div class="container" style="max-width: 950px; margin-left: auto; margin-right: auto;">
    <div class="container">
        <div class="section">
            <div class="embed-responsive embed-responsive-16by9">
                <iframe 
                width="560" 
                height="315" 
                src="https://www.youtube.com/embed/s1p_CvwDEB8" 
                frameborder="0" 
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                allowfullscreen></iframe>
            </div>
            <hr>
            <p>
                By processing audio signals in the time-domain with randomly weighted temporal convolutional networks (TCNs),
                we uncover a wide range of novel, yet controllable overdrive effects.
                We discover that architectural aspects, such as the depth of the network, 
                the kernel size, the number of channels, the activation function, as well as the weight initialization, 
                all have a clear impact on the sonic character of the resultant effect, without the need for training. 
                In practice, these effects range from conventional overdrive and distortion,
                to more extreme effects, as the receptive field grows, similar to a fusion of distortion, equalization, delay, and reverb.  
                To enable use by musicians and producers, we provide a real-time plugin implementation.
                This allows users to dynamically design networks, listening to the results in real-time.
            </p>
        </div>
    
        <div class="section">
            <h2>History</h2>
            <hr>
            <p>
                Distortion was first discovered by guitarists when they found that pushing their early vacuum tube guitar amplifiers beyond their operating range produced a warm and fuzzy sound.
                While distortion is often considered an undesirable artifact, musicians quickly discovered that they could take advantage of these limitations for creative effect. 
                Distortion went on to invade a number of different genres and ultimately became a defining characteristic of rock music.
                Even today, when many listeners think of the sound of an electric guitar, its a sound dominated by the overdriven characteristic of tube distortion.  
            </p>
            <div class="row  justify-content-center">
                <div class="col-md-6">
                    <a href="#">
                        <img class="img-fluid" src="img/old-amp-front.jpg" alt="inn_logo">
                    </a>
                </div>
                <div class="col-md-6">
                    <a href="#">
                        <img class="img-fluid" src="img/old-amp-back.jpg" alt="ccs_logo">
                    </a>
                </div>
                <blockquote class="blockquote">
                    <footer class="blockquote-footer">1940 National Dobro Corp Model 75 Vintage Tube Guitar Amplifier</footer>
                </blockquote>
            </div>

            <p>
                From a technical standpoint, the concept of distorting or clipping and audio signal is quite straightforward.  
                As shown in the diagram below, on the right, when the amplitude of the input signal (grey) passes a certain threshold, 
                the amplitude of the output (red) is restricted from increasing further.
                This seemingly basic process has the effect of introducing additional harmonics to the signal,
                modifying the timbre and overall characteristic of the original sound. 
                While this effect is considered undesirable in most contexts, such as the voice of a speaker on a phone call, 
                musicians have found applications of this effect that result in more energetic, engaging, and emotionally moving sounds.  
            </p>

            <div class="row justify-content-center">
                <div class="col-md-6">
                    <img class="img-fluid" src='img/hard_clipper.png'>
                    <blockquote class="blockquote">
                    <footer class="blockquote-footer">Diode clipper distortion circuit. Source: <a href=https://www.strymon.net/single-stage-multistage-gain-topologies-sunset-riverside/>Strymon</a></footer>
                    </blockquote>
                </div>

                <div class="col-md-6">
                    <img class="img-fluid" src="img/clipping.png" alt="inn_logo">
                    <blockquote class="blockquote">
                        <footer class="blockquote-footer">The input signal (grey) is clipped at the maximum and minium voltage afford by the amplifier, which results in a distorted signal (red). 
                            Source: <a href="https://sound.stackexchange.com/questions/23122/how-to-remove-sound-clipping-in-existing-record">StackExchange</a> </footer>
                    </blockquote>
                </div>  
            </div>

            <p>
                After the initial discovery of distortion effects, 
                guitarists and engineers began searching for new methods for purposefully inducing distortion.
                In some cases, guitarists even intentionally damaged the speakers or circuitry within their amplifiers. 
                With the introduction of transistors, dedicated distortion devices could be built to fit in small enclosures. 
                This led to the emergence of the guitar stompbox or pedal, a small enclosure with a dedicated circuit for distorting audio signals. 
            </p>

            <div class="row justify-content-center">
                <div class="col-md-6">
                    <p>
                        The circuit diagram above shows an example of one of the most simple circuits of this kind, 
                        where a pair of diodes are used to clip the top and bottom of the waveform. 
                        Designers found that by using different kinds of circuit components, or varying circuit topologies,
                        they could achieve a wider range of unique distortion timbres. 
                        Presently, guitarists and audio engineers are presented with a wide array of digital signal processing devices 
                        for applying a seemingly endless number of different distortion effects. 
                        We aim to extend the world of distortion-based effects by proposing the use of neural networks as a new method of generating new kinds of distortion effects. 
                    </p>
                </div>
               
                <div class="col-md-6">
                    <img class="img-fluid" src='img/pedals.jpg'>
                    <blockquote class="blockquote">
                    <footer class="blockquote-footer">A collection of distortion, overdrive, and boost pedals. Source: <a href="https://www.sweetwater.com/insync/boost-overdrive-distortion-fuzz-pedals-whats-the-difference/">Sweetwater</a></footer>
                    </blockquote>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Architecture</h2>
            <hr>
            <p>
                In our case, we view a neural network as a kind of circuit that can in fact distort audio signals. 
                The benefit in this approach is that our circuit offers far greatly complexity in the space of signal processing possibilities than traditional analog circuits.
                For every neural network architecture, we can randomly set the weights of each layer within the network to produce a different effect. 
                Additionally, by adjusting aspects of the architecture itself, such as adding more layers, changing the intermediate nonlinear functions, of changing the size of each layer,  
                This changes the interface we use to search for effects. 
                Instead of tweaking the type of effect we achieve by modifying potentiometers within a circuit, 
                or even changing the circuit components, 
                we provide controls that let users modify elements of the architecture.
            </p>  

            <div class="row justify-content-center">

                <div class="col-md-4">
                    <img class="img-fluid" class="center" src='neurips/figs/plugin-diagram-v2.svg'>
                </div>
                <div class="col-md-8">
                    <p>
                        In this work, we focus on the temporal convolutional network (TCN) architecture. 
                        This architecture has already been shown to be successful in modeling sequential data across a number of domains, 
                        with WaveNet being one of the early examples in the audio domain. 
                        More recent works have applied this architecture to a number of different tasks, such as speech denoising and source separation. 
                        In addition, they have been applied specifically to the task of emulating the characteristic distortion produced by different amplifiers and distortion circuits. 
                    </p>
                </div>
            </div>

        </div>

        <div class="section">
            <h2>Plugin</h2>
            <hr>
            <p>
                The goal of the plugin is to build a C++ implementation that enables users to quickly and easily construct different
                randomly weighted TCNs, and listen to the produced effects in real-time. 
                This shifts the process of searching for and selecting audio processing effects from the traditional paradigm 
                where users adjust the controls of traditional DSP devices like equalizers and dynamic range compressors,
                to one where users adjust the architecture of a neural network in order to search for new effects. 
                The TCN can be viewed as a generalized audio effect that can effectively implement a range of transforms,
                similar to those traditionally employed by audio engineers (e.g. equalization, dynamic range compression, delay, reverberation). 
            </p>

            <div class="row justify-content-center">
                <div class="col-md-8">
                    <p>
                        <a type="button" href="https://drive.google.com/file/d/15tA3X21N5FhLsDvElGBArUFA9cTogDLL/view?usp=sharing" class="btn btn-secondary btn-md">Download</a>
                        For details on the installation see the <a target="blank_" href="https://github.com/csteinmetz1/ronn">GitHub repository</a>.
                    </p>
                    <img class="img-fluid" class="center" src='neurips/figs/ronn-vst-ui.png'>
                    <blockquote class="blockquote">
                    <footer class="blockquote-footer">Screenshot of the plugin user-interface.</footer>
                    </blockquote>

                </div>
            </div>
        </div>
    
        <div class="mt-2 section">
            <h2>Related Projects</h2>
            <hr>
            <p>
                Check out our related projects on using neural networks for audio effects! <br>
            </p>
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='https://csteinmetz1.github.io/NeuralReverberator/slides/static/img/nr-rir.gif' class='img-fluid'>
                </div>
    
                <div class="col">
                    <div class='paper-title'>
                        <a href="https://www.christiansteinmetz.com/projects-blog/neuralreverberator">NeuralReverberator</a>
                    </div>
                    <div>
                        Adding reverberation to musical sources is an essential part of creating an engaging listening experience. 
                        NeuralReverberator is a convolutional reverb synthesizer that uses a spectral autoencoder as a generative model. 
                        We implement the model as a real-time plugin that allows audio engineers to traverse the latent space of the autoencoder,
                        and generate novel reverberation effect with varying timbre and duration. 
                    </div>
                </div>
            </div>
            <br>
            <div class='row vspace-top'>
                <div class="col-sm-3">
                    <img src='https://floweq.ml/img/tf.gif' class='img-fluid'>
                </div>
    
                <div class="col">
                    <div class='paper-title'>
                        <a href="http://floweq.ml">flowEQ</a>
                    </div>
                    <div>
                        We use a disentangled variational autoencoder (β-VAE) in order to provide a new modality for modifying the timbre of recordings via a parametric equalizer. 
                        By traversing the learned latent space of the trained decoder network in the real-time plugin, 
                        the user can more quickly search through the configurations of a five band parametric equalizer. 
                        This methodology promotes using one’s ears to determine the proper EQ settings over looking at transfer functions or specific frequency controls.
                    </div>
                </div>
            </div>
        </div>
        <br>
    
        <div class="section">
            <h2>Paper</h2>
            <hr>
            <div>
                <div class="list-group">
                    <a href=""
                       class="list-group-item">
                        <img src="neurips/figs/paper_thumbnail_sm.jpg" style="width:100%; margin-right:-20px; margin-top:-10px;">
                    </a>
                </div>
            </div>
        </div>
    
        <div class="section">
            <h2>Bibtex</h2>
            <hr>
            <div style="background-color: #e9ecef;">
                <pre>
                <code>
    @article{steinmetz2020overdrive,
            title={Randomized Overdrive Neural Networks},
            author={Steinmetz, Christian J. and Reiss, Joshua D.},
            journal={arXiv preprint arXiv:},
            year={2020}}</code>
                </pre>
            </div>
        </div>

        <!-- <div class="section">
            <h2 id="bibliography">Bibliography</h2>
            <hr>
        
            <div id="bibtex_display"></div>
            <div class="bibtex_template">
                <p>
                    <div class="if author">
                    <span class="author"></span>
                    </div>
                    <span class="if title">
                    <a class="url">
                        <span class="title"></span>
                    </a>
                    </span>
                    <div>
                    <span class="if journal"><em><span class="journal"></span></em></span>
                    <span class="if month"><span class="month"></span>,</span>
                    <span class="if year"><span class="year"></span>.</span>
                    </div>
                </p>
            </div>
        </div>  -->

        <hr>
        <footer>
            <p>*Submitted to the NeurIPS 2020 Workshop on Machine Learning for Creativity and Design</p>
            <p>Send feedback and questions to <a href="http://christiansteinmetz.com">Christian Steinmetz</a>.</p>
        </footer>
    </div>
</div>

<script type="text/javascript" src="https://cdn.rawgit.com/pcooksey/bibtex-js/5ccf967/src/bibtex_js.js"></script>
<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
